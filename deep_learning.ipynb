{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build my own deep learning functions\n",
    "\n",
    "## Purpose\n",
    "\n",
    "Learn by practicing. Notations follow Andrew Ng's Coursera deep learning course."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the forward_prop function\n",
    "Note: If there is error while importing python modules while running this notebook in vscode, make sure the both the vscode python interpreter and ipython kernal are both set properly. \n",
    "\n",
    "### Model structure\n",
    "The model consist of L-1 relu layers and one sigmoid layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "np.random.seed(1) \n",
    "# TODO: with certain seeds, e.g. seed=1, the cost generates NAN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below function taken from course assignments\n",
    "def load_data():\n",
    "    train_dataset = h5py.File('datasets/train_catvnoncat.h5', \"r\")\n",
    "    train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:])\n",
    "    train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:])\n",
    "\n",
    "    test_dataset = h5py.File('datasets/test_catvnoncat.h5', \"r\")\n",
    "    test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:])\n",
    "    test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:])\n",
    "\n",
    "    classes = np.array(test_dataset[\"list_classes\"][:])\n",
    "    \n",
    "    train_set_y_orig = train_set_y_orig.reshape((1, train_set_y_orig.shape[0]))\n",
    "    test_set_y_orig = test_set_y_orig.reshape((1, test_set_y_orig.shape[0]))\n",
    "    \n",
    "    return train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(Z):\n",
    "    '''\n",
    "    Z: Input to the activate function\n",
    "\n",
    "    A: Output of the relu function\n",
    "    '''\n",
    "    A = np.maximum(0, Z)\n",
    "    assert(np.min(A) >= 0.0)\n",
    "    return A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, Y, parameters):\n",
    "    '''\n",
    "    # TODO: What's the row # variable?\n",
    "    X: Input data. (n0???, m). n0: feature #; m: # of examples\n",
    "    Y: Labels. (1, m)\n",
    "    parameters: The model parameters\n",
    "\n",
    "    cost: The return\n",
    "    cache: The intermediate values\n",
    "    '''\n",
    "    L = len(parameters) // 2\n",
    "    Xl = X\n",
    "    m = Y.shape[1]\n",
    "    caches = {}\n",
    "    for l in range(1, L):\n",
    "        # print('l=', l)\n",
    "        W = parameters['W'+str(l)]\n",
    "        b = parameters['b'+str(l)]\n",
    "\n",
    "        assert(W.shape[0] == b.shape[0])\n",
    "        Z = np.dot(W, Xl) + b\n",
    "        A = relu(Z)\n",
    "        Xl = A\n",
    "        caches['A'+str(l)] = A\n",
    "\n",
    "    ZL = np.dot(parameters['W'+str(L)], Xl) + parameters['b'+str(L)]\n",
    "    AL = 1 / (1 + np.exp(-ZL)) # sigmoid\n",
    "    # print('ZL=', ZL)\n",
    "    # print('AL=', AL)\n",
    "    print('### debug ###: AL.max(), AL.min()=', AL.max(), AL.min())\n",
    "\n",
    "    J = - np.dot(Y, np.log(AL).T) - np.dot(1 - Y, np.log(1 - AL).T)\n",
    "    cost = np.squeeze(np.sum(J)) / m\n",
    "    assert(cost.shape == ())\n",
    "\n",
    "    return cost, caches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and pre-processing\n",
    "train_set_x_orig, train_set_y_orig, test_set_x_orig, test_set_y_orig, classes = load_data()\n",
    "# print('train_set_x_orig.shape=', train_set_x_orig.shape)\n",
    "\n",
    "# plt.imshow(train_set_x_orig[7])\n",
    "# plt.show()\n",
    "\n",
    "train_set_x_flatten = train_set_x_orig.reshape(train_set_x_orig.shape[0], -1).T\n",
    "test_set_x_flatten = test_set_x_orig.reshape(test_set_x_orig.shape[0], -1).T\n",
    "\n",
    "# print('train_set_x_flatten.shape=', train_set_x_flatten.shape)\n",
    "# print('test_set_x_flatten.shape=', test_set_x_flatten.shape)\n",
    "# print(train_set_x_flatten)\n",
    "# print(train_set_x_flatten.max())\n",
    "\n",
    "train_set_x = train_set_x_flatten / 255.0\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "Initialize the parameters\n",
    "\n",
    "### Lesson learned\n",
    "When the weights are not initailized small, aka, without *0.01, the cost computation gives lots of NAN because the output are either too small or too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(layer_dims):\n",
    "    parameters = {}\n",
    "    for i in range(1, len(layer_dims)):\n",
    "        parameters['W'+str(i)] = np.random.rand(layer_dims[i], layer_dims[i-1]) * 0.01 # the down-scaling is important\n",
    "        parameters['b'+str(i)] = np.zeros((layer_dims[i], 1))\n",
    "        # print(parameters['W'+str(i)].shape, parameters['b'+str(i)].shape)\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Back prop\n",
    "The back propogation is to calculate the partial derivatives of the cost w.r.t. all the weights and biases. So that the learning algorithem can nudge the weights and biases to reduce the cost by a tiny bit. \n",
    "\n",
    "#### Notations\n",
    "Following cousera's notation, Z1, Z2, etc. are the linear combination of the input with biases for layers 1, 2, and so on. A1, A2, etc. are the activation outputs for layers 1, 2, and so on.\n",
    "\n",
    "$dZ_1=\\frac{d cost}{d Z1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_prop(Y, parameters, caches):\n",
    "    '''\n",
    "    X: Input data. (n0???, m). n0: feature #; m: # of examples\n",
    "    Y: Labels. (1, m)\n",
    "    parameters: The model parameters\n",
    "\n",
    "    grads: return the gradients\n",
    "    '''\n",
    "    grads = {}\n",
    "    print('parameters[''W3''].shape=', parameters['W3'].shape)\n",
    "    print('parameters[''b3''].shape=', parameters['b3'].shape)\n",
    "    \n",
    "    # The 3rd layer is a sigmoid layer\n",
    "    grads['dZ3'] = caches['A3'] - Y\n",
    "    grads['dW3'] = parameters['W3'] * grads['dZ3']\n",
    "    grads['db3'] = grads['dZ3']\n",
    "\n",
    "    # The 2nd layer is a RELU layer\n",
    "    grads['dZ2'] = np.ones_like(caches['Z2']) * (caches['Z2'] > 0.0)\n",
    "    print('caches[Z2]=', caches['Z2'])\n",
    "    print('grads[Z2]=', grads['Z2'])\n",
    "    grads['dW2'] = parameters['W2']\n",
    "    \n",
    "    # The 1st layer is a RELU layer\n",
    "    grads['dZ1'] = np.ones_like(caches['Z1']) * (caches['Z1'] > 0.0)\n",
    "    print('caches[Z1]=', caches['Z1'])\n",
    "    print('grads[Z1]=', grads['Z1'])\n",
    "\n",
    "    # m = \n",
    "    # dAl = 1/m\n",
    "\n",
    "    # assert(len(grads) == ())\n",
    "\n",
    "    return grads\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The overall model\n",
    "Here is the overall learning model with hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(X, Y, layer_dims, number_of_iterations = 5, learning_rate = 0.01):\n",
    "    # model initialization\n",
    "    parameters = model_init(layer_dims)\n",
    "\n",
    "    # results\n",
    "    costs = np.zeros((number_of_iterations, 1))\n",
    "\n",
    "    for i in range(number_of_iterations):\n",
    "        \n",
    "        costs[i], caches = forward_prop(train_set_x, train_set_y_orig, parameters)\n",
    "        # print('len(train_set_y_orig)=', len(train_set_y_orig))\n",
    "        # print('train_set_y_orig.shape', train_set_y_orig.shape)\n",
    "        # print(tmp)\n",
    "        \n",
    "        grads = back_prop(train_set_y_orig, parameters, caches)\n",
    "\n",
    "        parameters = parameters - grads * learning_rate\n",
    "\n",
    "        print('i=', i)\n",
    "\n",
    "    return parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### debug ###: AL.max(), AL.min()= 0.5064198968292074 0.500488435708394\n",
      "parameters[W3].shape= (1, 4)\n",
      "parameters[b3].shape= (1, 1)\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'A3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m layer_dims \u001b[39m=\u001b[39m [train_set_x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m8\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m1\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m model \u001b[39m=\u001b[39m train_model(train_set_x, train_set_y_orig, layer_dims, \u001b[39m2\u001b[39;49m)\n\u001b[0;32m      3\u001b[0m \u001b[39m# predict(model, test_set_x_orig, test_set_y_orig)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[18], line 15\u001b[0m, in \u001b[0;36mtrain_model\u001b[1;34m(X, Y, layer_dims, number_of_iterations, learning_rate)\u001b[0m\n\u001b[0;32m     10\u001b[0m costs[i], caches \u001b[39m=\u001b[39m forward_prop(train_set_x, train_set_y_orig, parameters)\n\u001b[0;32m     11\u001b[0m \u001b[39m# print('len(train_set_y_orig)=', len(train_set_y_orig))\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m# print('train_set_y_orig.shape', train_set_y_orig.shape)\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m# print(tmp)\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m grads \u001b[39m=\u001b[39m back_prop(train_set_y_orig, parameters, caches)\n\u001b[0;32m     17\u001b[0m parameters \u001b[39m=\u001b[39m parameters \u001b[39m-\u001b[39m grads \u001b[39m*\u001b[39m learning_rate\n\u001b[0;32m     19\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mi=\u001b[39m\u001b[39m'\u001b[39m, i)\n",
      "Cell \u001b[1;32mIn[17], line 14\u001b[0m, in \u001b[0;36mback_prop\u001b[1;34m(Y, parameters, caches)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mparameters[\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39mb3\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m\u001b[39m].shape=\u001b[39m\u001b[39m'\u001b[39m, parameters[\u001b[39m'\u001b[39m\u001b[39mb3\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mshape)\n\u001b[0;32m     13\u001b[0m \u001b[39m# The 3rd layer is a sigmoid layer\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m grads[\u001b[39m'\u001b[39m\u001b[39mdZ3\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m caches[\u001b[39m'\u001b[39;49m\u001b[39mA3\u001b[39;49m\u001b[39m'\u001b[39;49m] \u001b[39m-\u001b[39m Y\n\u001b[0;32m     15\u001b[0m grads[\u001b[39m'\u001b[39m\u001b[39mdW3\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m parameters[\u001b[39m'\u001b[39m\u001b[39mW3\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m*\u001b[39m grads[\u001b[39m'\u001b[39m\u001b[39mdZ3\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m     16\u001b[0m grads[\u001b[39m'\u001b[39m\u001b[39mdb3\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m grads[\u001b[39m'\u001b[39m\u001b[39mdZ3\u001b[39m\u001b[39m'\u001b[39m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'A3'"
     ]
    }
   ],
   "source": [
    "layer_dims = [train_set_x.shape[0], 8, 4, 1]\n",
    "model = train_model(train_set_x, train_set_y_orig, layer_dims, 2)\n",
    "# predict(model, test_set_x_orig, test_set_y_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_relu_grad= [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n",
      "gate= [[0.93339344]\n",
      " [0.52109101]\n",
      " [0.96513247]\n",
      " [0.62705207]\n",
      " [0.75136111]]\n",
      "test_relu_grad= [[1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]\n",
      " [1.]]\n"
     ]
    }
   ],
   "source": [
    "gate = np.random.rand(5, 1)\n",
    "test_relu_grad = np.ones_like(gate)\n",
    "test_relu_grad = test_relu_grad * (gate > 0.5)\n",
    "print('test_relu_grad=', test_relu_grad)\n",
    "print('gate=', gate)\n",
    "print('test_relu_grad=', test_relu_grad)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "879bf236243f5f3c3f16927df85609c2f6dbedfe1e86c1d37000aa722c5f092b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
